defaults:
  - grpo
  - reward_cfg@_global_: teacher_logprob_kl
  - _self_

trainer_log_name: teacher_gspo_rw_${reward_log_name}

# GSPO specific - this is the key change!
importance_sampling_level: "sequence"

reward_fns:
  _target_: hydra_utils.wrap_as_list
  teacher_reward: ${teacher_reward}

logging_prob: 0.1
student_model: null
use_reference_teacher_model: false
student_model_init_kwargs: null
completion_only_training: false
disable_student_offloading: false

# Root-level switch referenced inside trainer_args
shuffle_dataset: true

# Override the trainer_args to include importance_sampling_level
trainer_args:
  _target_: trl.trainer.grpo_config.GRPOConfig
  importance_sampling_level: ${importance_sampling_level}
  model_init_kwargs: {}
  remove_unused_columns: ${remove_unused_columns}
  max_prompt_length: ${max_prompt_length}
  num_generations: ${num_generations}
  max_completion_length: ${max_completion_length}
  shuffle_dataset: ${shuffle_dataset}
  ds3_gather_for_generation: false
  temperature: ${temperature}
  top_p: ${top_p}
  top_k: ${top_k}
  min_p: ${min_p}
  repetition_penalty: ${repetition_penalty}
  use_vllm: ${use_vllm}
  vllm_gpu_memory_utilization: ${vllm_gpu_memory_utilization}
  # removed unsupported ray* and vllm_sleep_level
  vllm_server_port: ${vllm_server_port}
  generation_batch_size: ${generation_batch_size}
  steps_per_generation: ${steps_per_generation}
  learning_rate: ${learning_rate}
  beta: ${beta}
  loss_type: bnpo
  reward_weights: ${reward_weights}
  sync_ref_model: ${sync_ref_model}
  ref_model_mixup_alpha: ${ref_model_mixup_alpha}
  ref_model_sync_steps: ${ref_model_sync_steps}
  log_completions: ${log_completions}

trainer:
  _target_: trainers.TeacherGRPOTrainer
  student_model: ${student_model}
  use_reference_teacher_model: ${use_reference_teacher_model}
  student_model_init_kwargs: ${student_model_init_kwargs}
  logging_prob: ${logging_prob}
  disable_student_offloading: ${disable_student_offloading}