defaults:
  - grpo
  - reward_cfg@_global_: teacher_logprob_kl
  - _self_

trainer_log_name: teacher_gspo_rw_${reward_log_name}

# GSPO specific - this is the key change!
importance_sampling_level: "sequence"

reward_fns:
  _target_: hydra_utils.wrap_as_list
  teacher_reward: ${teacher_reward}

logging_prob: 0.1
student_model: null
use_reference_teacher_model: false
student_model_init_kwargs: null
completion_only_training: false
disable_student_offloading: false

# Override the trainer_args to include importance_sampling_level
trainer_args:
  _target_: trainers.GRPOConfig
  importance_sampling_level: ${importance_sampling_level}
  model_init_kwargs: ${model_init_kwargs}
  remove_unused_columns: ${remove_unused_columns}
  max_prompt_length: ${max_prompt_length}
  num_generations: ${num_generations}
  max_completion_length: ${max_completion_length}
  shuffle_generation_inputs: ${shuffle_generation_inputs}
  ds3_gather_for_generation: ${ds3_gather_for_generation}
  temperature: ${temperature}
  top_p: ${top_p}
  top_k: ${top_k}
  min_p: ${min_p}
  repetition_penalty: ${repetition_penalty}
  generation_aggregation_steps: ${generation_aggregation_steps}
  use_vllm: ${use_vllm}
  vllm_device: ${vllm_device}
  vllm_gpu_memory_utilization: ${vllm_gpu_memory_utilization}
  vllm_dtype: ${vllm_dtype}
  vllm_max_model_len: ${vllm_max_model_len}
  use_ray: ${use_ray}
  ray_share_training_devices: ${ray_share_training_devices}
  ray_tensor_parallelism: ${ray_tensor_parallelism}
  ray_data_parallelism: ${ray_data_parallelism}
  ray_no_memory_duplication: ${ray_no_memory_duplication}
  enable_prefix_caching: ${enable_prefix_caching}
  enforce_eager: ${enforce_eager}
  vllm_sleep_level: ${vllm_sleep_level}
  use_vllm_server: ${use_vllm_server}
  vllm_host: ${vllm_host}
  vllm_port: ${vllm_port}
  vllm_group_port: ${vllm_group_port}
  num_vllm_clients: ${num_vllm_clients}
  learning_rate: ${learning_rate}
  beta: ${beta}
  reward_weights: ${reward_weights}
  sync_ref_model: ${sync_ref_model}
  ref_model_mixup_alpha: ${ref_model_mixup_alpha}
  ref_model_sync_steps: ${ref_model_sync_steps}
  log_completions: ${log_completions}
  save_completions_probability: ${save_completions_probability}
  artificial_epochs: ${artificial_epochs}
  backprop_accumulation_steps: ${backprop_accumulation_steps}
  backprop_accumulation_micro_batch_size: ${backprop_accumulation_micro_batch_size}
  offload_untrained_models: ${offload_untrained_models}
  unbias_log_probabilities: ${unbias_log_probabilities}
  activate_debugging_logs: ${activate_debugging_logs}
  push_to_hub: ${push_to_hub}

trainer:
  _target_: trainers.TeacherGRPOTrainer
  student_model: ${student_model}
  use_reference_teacher_model: ${use_reference_teacher_model}
  student_model_init_kwargs: ${student_model_init_kwargs}
  logging_prob: ${logging_prob}
  disable_student_offloading: ${disable_student_offloading}